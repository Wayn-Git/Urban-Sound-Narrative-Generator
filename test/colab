# Google Colab Backend Setup for Sound Narrative API
# Run this in Google Colab to expose your FastAPI backend

# ============== STEP 1: Install Dependencies ==============
print("üì¶ Installing dependencies...")
!pip install -q fastapi uvicorn pydantic python-dotenv
!pip install -q torch torchaudio --index-url https://download.pytorch.org/whl/cpu
!pip install -q openai-whisper
!pip install -q panns-inference
!pip install -q groq
!pip install -q pydub
!pip install -q pyngrok

print("‚úÖ Dependencies installed!")

# ============== STEP 2: Pre-download Models ==============
print("\nü§ñ Pre-downloading AI models...")
from google.colab import drive
drive.mount('/content/drive', force_remount=True)  # Mount Drive (auth prompt)
import whisper
import torch
import os
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'
download_root = '/content/drive/MyDrive/.cache/whisper'  # Persistent
try:
    print("Downloading Whisper base model to Drive...")
    model_url = "https://openaipublic.azureedge.net/main/whisper/models/ed3a0b6b1c0edf879ad9b11b1af5a0e6ab5db9205f891f668f8b0e6c6326e34e/base.pt"
    os.makedirs(download_root, exist_ok=True)
    import urllib.request
    filename = os.path.join(download_root, 'base.pt')
    urllib.request.urlretrieve(model_url, filename)
    model_size = os.path.getsize(filename)
    print(f"‚úÖ Whisper base model downloaded to Drive ({model_size / (1024*1024):.1f} MB)")
    whisper_model = whisper.load_model("base", device='cpu', download_root=download_root)
    del whisper_model
    print("‚úÖ Model verified!")
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
except Exception as e:
    print(f"‚ö†Ô∏è Warning: Could not pre-download Whisper model: {e}")
    print("The model will be downloaded on first use (or use fallback).")

# ============== STEP 3: Setup Environment Variables ==============
print("\nüîë Setting up environment variables...")
import os
from google.colab import userdata

# IMPORTANT: Store these as Colab Secrets
# Go to: üîë (key icon) in left sidebar ‚Üí Add new secret
try:
    os.environ["GROQ_API_KEY"] = userdata.get('GROQ_API_KEY')
    os.environ["ELEVENLABS_API_KEY"] = userdata.get('ELEVENLABS_API_KEY')
    print("‚úÖ API keys loaded from Colab secrets")
except Exception as e:
    print("‚ùå Error loading secrets. Please add GROQ_API_KEY and ELEVENLABS_API_KEY to Colab secrets")
    print(f"Error: {e}")

# ============== STEP 4: Setup Ngrok ==============
print("\nüåê Setting up ngrok tunnel...")
from pyngrok import ngrok, conf
import nest_asyncio

# Enable nested async for Colab
nest_asyncio.apply()

# Get ngrok auth token from Colab secrets
try:
    NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')
    conf.get_default().auth_token = NGROK_AUTH_TOKEN
    print("‚úÖ Ngrok authenticated")
except:
    print("‚ö†Ô∏è  Warning: NGROK_AUTH_TOKEN not found in secrets")
    print("   Get free token from: https://dashboard.ngrok.com/get-started/your-authtoken")
    print("   Add it to Colab secrets as 'NGROK_AUTH_TOKEN'")

# ============== STEP 5: Backend Code ==============

print("\nüìù Creating backend v2.0.4 with reordered cleanup...")
from fastapi import BackgroundTasks
backend_code = '''
import os
from dotenv import load_dotenv
from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, StreamingResponse
from pydantic import BaseModel
from contextlib import asynccontextmanager
import torch
import torchaudio
import whisper
from panns_inference import AudioTagging, labels
import requests
from groq import Groq
from pydub import AudioSegment
from pathlib import Path
import warnings
import uuid
import json
import asyncio
import logging
from typing import List, Tuple
import time
import re
# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
warnings.filterwarnings("ignore", category=SyntaxWarning, module="pydub")
# Load environment variables
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")
# Global model storage
models = {}
# ==================== CONFIGURATION ====================
GROQ_MODEL_CONFIG = {
    "model": "llama-3.3-70b-versatile",
    "temperature": 0.7,
    "max_tokens": 150,
    "top_p": 0.9,
    "frequency_penalty": 0.3,
    "presence_penalty": 0.2
}
ELEVENLABS_V3_CONFIG = {
    "voice_id": "cgSgspJ2msm6clMCkdW9",
    "model_id": "eleven_turbo_v2_5",
    "voice_settings": {
        "stability": 0.4,
        "similarity_boost": 0.75,
        "style": 0.8,
        "use_speaker_boost": True
    },
    "output_format": "mp3_44100_128"
}
# ==================== RESPONSE MODELS ====================
class ProcessingUpdate(BaseModel):
    stage: str
    message: str
    sounds: List[str] = []
    progress: int
class AudioResponse(BaseModel):
    narration: str
    audio_url: str
    detected_sounds: List[str]
    transcript: str
# ==================== HELPER: LAZY WHISPER LOADER ====================
async def load_whisper_lazy():
    if 'whisper' in models and models['whisper'] is not None:
        return models['whisper']
    
    max_retries = 3
    retry_delay = 10
    for attempt in range(max_retries):
        try:
            logger.info(f"Lazy-loading Whisper model (attempt {attempt + 1}/{max_retries})...")
            models['whisper'] = whisper.load_model("base", device='cpu', download_root='/root/.cache/whisper')
            logger.info("Whisper model loaded successfully")
            return models['whisper']
        except Exception as e:
            logger.warning(f"Failed to load Whisper (attempt {attempt + 1}): {str(e)}")
            if attempt < max_retries - 1:
                logger.info(f"Retrying in {retry_delay} seconds...")
                await asyncio.sleep(retry_delay)
            else:
                logger.error(f"Whisper load failed after {max_retries} attempts: {str(e)}")
                models['whisper'] = None
                return None
# ==================== APP INITIALIZATION ====================
@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("Loading AI models...")
   
    try:
        models['panns'] = AudioTagging(checkpoint_path=None, device='cpu')
        logger.info("PANNs model loaded successfully")
    except Exception as e:
        logger.error(f"Failed to load PANNs model: {str(e)}")
        raise
   
    try:
        await load_whisper_lazy()
    except Exception as e:
        logger.warning(f"Initial Whisper load failed (will retry lazily): {str(e)}")
        models['whisper'] = None
   
    os.makedirs("audio", exist_ok=True)
    logger.info("Models setup complete. Server ready! (Whisper may load on first request)")
    yield
    models.clear()
    logger.info("Application shutdown complete")

app = FastAPI(
    title="Urban Sound Narrative API",
    description="AI-Powered Audio Processing for Emotionally Rich Narration",
    version="2.0.4",  # Bumped for reorder + delay
    lifespan=lifespan
)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
# ==================== HELPER FUNCTIONS ====================
def build_optimized_groq_prompt(sound_context: str, transcript: str) -> str:
    return (
        f"You are a master storyteller crafting a cinematic urban narrative. "
        f"Based on these detected sounds: {sound_context}, write a vivid, "
        f"emotionally evocative 2-3 sentence scene that immerses the reader in a bustling city moment.\\n\\n"
        f"CRITICAL REQUIREMENTS:\\n"
        f"1. Seamlessly integrate this spoken phrase into the scene: '{transcript}'\\n"
        f"2. Use square brackets to mark emotional delivery ONLY for dialogue, like: "
        f"[enthusiastically], [wearily], [sarcastically], [thoughtfully]\\n"
        f"3. Make the scene feel alive - use sensory details, movement, and atmosphere\\n"
        f"4. DO NOT include sound effect descriptions like (car honking) or *dog barks*\\n"
        f"5. Focus on human moments and urban poetry\\n\\n"
        f"Example style: 'The afternoon pulse of the city beats steadily as vendors call out their wares. "
        f"A passerby mutters [sarcastically], \\"Another beautiful day in paradise,\\" while dodging between rushing commuters.'"
    )
def build_optimized_elevenlabs_prompt(narration_text: str, sound_context: str) -> str:
    return f"""<voice>{narration_text}</voice>
<style>
Deliver this urban narrative with genuine emotion and theatrical flair. Use dynamic pacing: speak deliberately during descriptive passages to build atmosphere, then shift to natural conversational energy for any dialogue. Infuse warmth and immersion into environmental descriptions. For bracketed emotional cues like [enthusiastically] or [thoughtfully], embody that emotion fully - don't just read the word, become it. Vary your pitch and intensity to match the scene's energy. Make the listener feel present in this urban moment.
</style>
<context>
This is a vivid narration of an urban soundscape featuring: {sound_context}. The scene should feel cinematic and emotionally resonant, transporting the listener into a real city moment with all its texture and humanity.
</context>"""
async def generate_narrative_text(sound_types: List[str], transcript: str) -> Tuple[str, str]:
    try:
        sound_context = ", ".join(sound_types)
        logger.info(f"Generating narrative for sounds: {sound_context}")
       
        if not GROQ_API_KEY:
            raise HTTPException(status_code=500, detail="GROQ_API_KEY not configured")
       
        client = Groq(api_key=GROQ_API_KEY)
        prompt = build_optimized_groq_prompt(sound_context, transcript)
       
        response = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            **GROQ_MODEL_CONFIG
        )
       
        narration = response.choices[0].message.content.strip()
        if not narration:
            raise ValueError("GROQ returned empty narration")
       
        logger.info(f"Generated narration ({len(narration)} chars)")
        return narration, sound_context
       
    except Exception as e:
        logger.error(f"GROQ API error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to generate narration: {str(e)}")
async def synthesize_audio(narration_text: str, sound_context: str, output_filename: str) -> str:
    try:
        if not ELEVENLABS_API_KEY:
            raise HTTPException(status_code=500, detail="ELEVENLABS_API_KEY not configured")
       
        elevenlabs_prompt = build_optimized_elevenlabs_prompt(narration_text, sound_context)
       
        url = f"https://api.elevenlabs.io/v1/text-to-speech/{ELEVENLABS_V3_CONFIG['voice_id']}"
        headers = {
            "xi-api-key": ELEVENLABS_API_KEY,
            "Content-Type": "application/json",
            "Accept": "audio/mpeg"
        }
        payload = {
            "text": elevenlabs_prompt,
            "model_id": ELEVENLABS_V3_CONFIG["model_id"],
            "voice_settings": ELEVENLABS_V3_CONFIG["voice_settings"],
            "output_format": ELEVENLABS_V3_CONFIG["output_format"]
        }
       
        logger.info(f"Synthesizing speech with V3 (voice: {ELEVENLABS_V3_CONFIG['voice_id']})")
       
        tts_response = requests.post(url, headers=headers, json=payload, timeout=45)
       
        if tts_response.status_code != 200:
            error_detail = tts_response.text
            logger.error(f"ElevenLabs API error [{tts_response.status_code}]: {error_detail}")
            raise HTTPException(
                status_code=tts_response.status_code,
                detail=f"ElevenLabs TTS failed: {error_detail}"
            )
       
        logger.info(f"TTS synthesis successful ({len(tts_response.content)} bytes)")
       
        output_path = f"audio/{output_filename}"
        temp_path = f"audio/temp_{output_filename}"
       
        with open(temp_path, "wb") as f:
            f.write(tts_response.content)
       
        logger.info("Applying audio normalization and enhancement")
       
        audio = AudioSegment.from_mp3(temp_path)
        audio = audio + 6
        audio = audio.normalize()
        audio = audio.fade_in(100).fade_out(200)
       
        audio.export(output_path, format="mp3", bitrate="192k", parameters=["-q:a", "0"])
       
        Path(temp_path).unlink(missing_ok=True)
        logger.info(f"Audio exported to {output_path}")
       
        return output_path
       
    except requests.exceptions.Timeout:
        logger.error("ElevenLabs API timeout")
        raise HTTPException(status_code=504, detail="Text-to-speech service timed out. Please try again.")
    except requests.exceptions.RequestException as e:
        logger.error(f"ElevenLabs request error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"TTS request failed: {str(e)}")
    except Exception as e:
        logger.error(f"Audio processing error: {str(e)}")
        Path(f"audio/temp_{output_filename}").unlink(missing_ok=True)
        raise HTTPException(status_code=500, detail=f"Audio processing failed: {str(e)}")
# ==================== STREAMING PROCESSOR ====================
async def process_audio_file_streaming(audio_path: str, output_filename: str):
    try:
        if not os.path.exists(audio_path):
            logger.warning(f"Upload file vanished early: {audio_path} - using empty waveform fallback")
            waveform = torch.zeros(1, 32000 * 5)  # 5s silence placeholder
            sr = 32000
        else:
            file_size = os.path.getsize(audio_path)
            logger.info(f"Processing audio: {audio_path} ({file_size} bytes)")
        
        yield json.dumps({"stage": "loading", "message": "üéß Analyzing your audio...", "sounds": [], "progress": 10}) + "\\n"
        await asyncio.sleep(0.5)
       
        # Load waveform early for PANNs (keeps path intact for Whisper)
        waveform, sr = torchaudio.load(audio_path)
        if sr != 32000:
            waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=32000)(waveform)
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
        waveform = waveform.to(device)
        
        yield json.dumps({"stage": "extracting", "message": "üîä Extracting sound patterns...", "sounds": [], "progress": 25}) + "\\n"
        await asyncio.sleep(0.5)
        yield json.dumps({"stage": "identifying", "message": "üéµ Identifying urban soundscapes...", "sounds": [], "progress": 40}) + "\\n"
       
        clipwise_output, _ = models['panns'].inference(waveform)
        clipwise_output = clipwise_output.squeeze()
        top_indices = clipwise_output.argsort()[-5:][::-1]
        sound_types = [labels[int(i)] for i in top_indices]
        yield json.dumps({"stage": "sounds_detected", "message": "‚ú® Sounds detected!", "sounds": sound_types, "progress": 50}) + "\\n"
        await asyncio.sleep(0.8)
       
        # Now transcribe (path still exists)
        yield json.dumps({"stage": "transcribing", "message": "üëÇ Listening to speech...", "sounds": sound_types, "progress": 60}) + "\\n"
        whisper_model = await load_whisper_lazy()
        if whisper_model is None:
            logger.warning("Whisper unavailable - using fallback transcript")
            transcript = "(speech detected but transcription unavailable due to network issue)"
            yield json.dumps({"stage": "transcribe_warn", "message": "‚ö†Ô∏è Transcription fallback (network retry later)", "sounds": sound_types, "progress": 65}) + "\\n"
        else:
            result = whisper_model.transcribe(audio_path)
            transcript = result["text"].strip() or "(no speech detected)"
            yield json.dumps({"stage": "transcribe_complete", "message": "‚úÖ Speech transcribed", "sounds": sound_types, "progress": 65}) + "\\n"
        
        # Safe to unlink now (both waveform & transcribe done)
        Path(audio_path).unlink(missing_ok=True)
        logger.info("Temporary upload file cleaned up after processing")
       
        yield json.dumps({"stage": "ai_processing", "message": "ü§ñ Crafting cinematic narrative...", "sounds": sound_types, "progress": 70}) + "\\n"
        await asyncio.sleep(0.5)
        narration, sound_context = await generate_narrative_text(sound_types, transcript)
        yield json.dumps({"stage": "narrating", "message": "‚úçÔ∏è Preparing emotional delivery...", "sounds": sound_types, "progress": 80}) + "\\n"
        await asyncio.sleep(0.5)
        yield json.dumps({"stage": "voice_generation", "message": "üéôÔ∏è Generating expressive narration...", "sounds": sound_types, "progress": 90}) + "\\n"
        output_path = await synthesize_audio(narration, sound_context, output_filename)
        yield json.dumps({"stage": "finalizing", "message": "‚ú® Adding final touches...", "sounds": sound_types, "progress": 95}) + "\\n"
        await asyncio.sleep(0.5)
        audio_url = f"/audio/{output_filename}"
        yield json.dumps({
            "stage": "complete",
            "message": "‚úÖ Complete!",
            "sounds": sound_types,
            "progress": 100,
            "narration": narration,
            "audio_url": audio_url,
            "transcript": transcript
        }) + "\\n"
    except Exception as e:
        logger.error(f"Streaming processing error: {str(e)}")
        yield json.dumps({"stage": "error", "message": f"‚ùå Error: {str(e)}", "sounds": [], "progress": 0}) + "\\n"
# ==================== API ENDPOINTS ====================
@app.post("/process-audio-stream")
async def process_audio_stream(background_tasks: BackgroundTasks, file: UploadFile = File(...)):
    if not file.filename.endswith((".mp3", ".wav")):
        raise HTTPException(status_code=400, detail="Only MP3 or WAV files allowed")
   
    unique_id = str(uuid.uuid4())
    safe_filename = re.sub(r'[^a-zA-Z0-9._-]', '_', file.filename)
    output_filename = f"output_{unique_id}.mp3"
    audio_path = f"audio/upload_{unique_id}_{safe_filename}"
   
    try:
        content = await file.read()
        with open(audio_path, "wb") as f:
            f.write(content)
        logger.info(f"Uploaded {len(content)} bytes to {audio_path}")
       
        # Delayed cleanup for output MP3 (120s for safe download)
        async def cleanup_output():
            await asyncio.sleep(120)
            output_path = f"audio/{output_filename}"
            if os.path.exists(output_path):
                Path(output_path).unlink()
                logger.info(f"Output cleaned: {output_path}")
        background_tasks.add_task(cleanup_output)
       
        return StreamingResponse(
            process_audio_file_streaming(audio_path, output_filename),
            media_type="text/event-stream"
        )
   
    except Exception as e:
        logger.error(f"Streaming endpoint error: {str(e)}")
        Path(audio_path).unlink(missing_ok=True)
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")
@app.get("/audio/{filename}")
async def serve_audio(filename: str):
    file_path = f"audio/{filename}"
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="Audio file not found")
    return FileResponse(file_path, media_type="audio/mpeg")
@app.get("/")
async def root():
    return {
        "status": "healthy",
        "message": "Urban Sound Narrative API v2.0.4 (Reorder + Delay Fix)",
        "version": "2.0.4",
        "optimizations": [
            "GROQ Llama 3.3 70B with creative parameters",
            "ElevenLabs V3 Alpha emotional prompting",
            "Enhanced audio post-processing",
            "Lazy Whisper loading for network resilience",
            "Filename sanitization for FFmpeg compatibility",
            "Reordered file handling (waveform/PANNs ‚Üí Whisper ‚Üí unlink)",
            "Extended output cleanup (120s delay)"
        ]
    }
@app.get("/health")
async def health_check():
    whisper_status = "loaded" if models.get('whisper') is not None else "lazy-pending"
    return {
        "status": "healthy",
        "models_loaded": len([k for k, v in models.items() if v is not None]),
        "device": device,
        "groq_configured": bool(GROQ_API_KEY),
        "groq_model": GROQ_MODEL_CONFIG["model"],
        "elevenlabs_configured": bool(ELEVENLABS_API_KEY),
        "elevenlabs_model": ELEVENLABS_V3_CONFIG["model_id"],
        "voice_id": ELEVENLABS_V3_CONFIG["voice_id"],
        "whisper_status": whisper_status
    }
os.makedirs("audio", exist_ok=True)
'''
with open('main.py', 'w') as f:
    f.write(backend_code)
print("‚úÖ Backend v2.0.4 created with fixes!")

# ============== STEP 6: Start Server with Ngrok ==============
print("\nüöÄ Starting FastAPI server with ngrok tunnel...")

# Kill any existing ngrok tunnels
ngrok.kill()

# Start ngrok tunnel
public_url = ngrok.connect(8000, bind_tls=True)

# Extract the clean URL
import re
url_match = re.search(r'https://[a-z0-9]+\.ngrok-free\.app', str(public_url))
clean_url = url_match.group(0) if url_match else str(public_url)

print(f"\n{'='*60}")
print(f"üåê PUBLIC URL: {clean_url}")
print(f"{'='*60}")
print(f"\nüìã COPY THIS URL TO YOUR FRONTEND:")
print(f"   const API_URL = '{clean_url}';")
print(f"\n{'='*60}\n")

# Start FastAPI server
print("üîÑ Starting server (this may take 1-2 minutes for model loading)...\n")
!uvicorn main:app --host 0.0.0.0 --port 8000