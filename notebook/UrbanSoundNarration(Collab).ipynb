{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SKbMNS_8RiI"
      },
      "outputs": [],
      "source": [
        "audio_path = \"/content/city_sounds.mp3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvXuymoj8Y2g",
        "outputId": "1b140325-b476-457f-a36a-631f70812e21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting panns-inference\n",
            "  Downloading panns_inference-0.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from panns-inference) (3.10.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from panns-inference) (0.11.0)\n",
            "Collecting torchlibrosa (from panns-inference)\n",
            "  Downloading torchlibrosa-0.1.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa->panns-inference) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->panns-inference) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->panns-inference) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->panns-inference) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa->panns-inference) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa->panns-inference) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->panns-inference) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns-inference) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns-inference) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns-inference) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns-inference) (2025.10.5)\n",
            "Downloading panns_inference-0.1.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading torchlibrosa-0.1.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: torchlibrosa, panns-inference\n",
            "Successfully installed panns-inference-0.1.1 torchlibrosa-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install panns-inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR8imHVv8HYU",
        "outputId": "e7292f2d-8b6b-4f76-ce42-f9e44c14b072"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint path: /root/panns_data/Cnn14_mAP=0.431.pth\n",
            "GPU number: 1\n"
          ]
        }
      ],
      "source": [
        "# Feature extraction using PANNs (CNN14) pretrained model\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "from panns_inference import AudioTagging, SoundEventDetection, labels\n",
        "\n",
        "# Initialize the pretrained model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = AudioTagging(checkpoint_path=None, device=device)  # CNN14 pretrained on AudioSet\n",
        "\n",
        "def extract_panns_features(audio_path, target_sr=32000):\n",
        "    \"\"\"\n",
        "    Extracts powerful embeddings from audio using PANNs CNN14.\n",
        "    Args:\n",
        "        audio_path (str): path to audio file\n",
        "        target_sr (int): sampling rate (CNN14 expects 32kHz)\n",
        "    Returns:\n",
        "        embeddings (np.ndarray): 2048-dim feature vector representing the clip\n",
        "        predicted_tags (list): top predicted sound tags (optional)\n",
        "    \"\"\"\n",
        "    # 1. Load audio and resample to 32kHz\n",
        "    waveform, sr = torchaudio.load(audio_path)\n",
        "    if sr != target_sr:\n",
        "        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)(waveform)\n",
        "\n",
        "    # 2. Convert to mono if stereo\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "    waveform = waveform.to(device)\n",
        "\n",
        "    clipwise_output, embedding = model.inference(waveform)\n",
        "\n",
        "# Pool embeddings across time\n",
        "    pooled_embedding = embedding.mean(axis=0)  # already a NumPy array\n",
        "\n",
        "# Make clipwise_output 1D\n",
        "    clipwise_output = clipwise_output.squeeze()  # shape -> [num_classes]\n",
        "\n",
        "# Get top predicted tags\n",
        "    top_indices = clipwise_output.argsort()[-5:][::-1]  # top 5 indices\n",
        "    predicted_tags = [labels[int(i)] for i in top_indices]  # cast to int\n",
        "\n",
        "    return pooled_embedding, predicted_tags\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzMuMshWAFj0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viB8y0QF8epZ",
        "outputId": "7d826fc0-7ce6-4e3f-ffb8-b8e206abf51c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0.        , 0.        , 0.        , ..., 0.        , 0.18207598,\n",
              "        0.        ], dtype=float32),\n",
              " ['Speech',\n",
              "  'Vehicle',\n",
              "  'Car',\n",
              "  'Outside, urban or manmade',\n",
              "  'Traffic noise, roadway noise'])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_panns_features(audio_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrPg1ZZbEgvG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KapsfejVEhFO",
        "outputId": "6c4ded88-9de5-4488-af2a-b8fe49492db9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ PyTorch: 2.8.0+cu126\n",
            "✓ CUDA: True\n",
            "✓ Whisper installed\n",
            "✓ PANNs installed\n",
            "\n",
            "✅ Core packages are ready!\n"
          ]
        }
      ],
      "source": [
        "# Test what's already installed\n",
        "import torch\n",
        "print(f\"✓ PyTorch: {torch.__version__}\")\n",
        "print(f\"✓ CUDA: {torch.cuda.is_available()}\")\n",
        "\n",
        "import whisper\n",
        "print(f\"✓ Whisper installed\")\n",
        "\n",
        "from panns_inference import AudioTagging\n",
        "print(f\"✓ PANNs installed\")\n",
        "\n",
        "print(\"\\n✅ Core packages are ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKSnCqmhE_G_"
      },
      "outputs": [],
      "source": [
        "!pip un"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTGtFDbYFBOi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "648jdj79Ewpf",
        "outputId": "a46aa94c-cdab-4efb-a211-4d1289e90996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping whisper as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: openai-whisper 20250625\n",
            "Uninstalling openai-whisper-20250625:\n",
            "  Successfully uninstalled openai-whisper-20250625\n",
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=64026f5a9ad08281c0bb3dbc09b488ba6cf4eaa4dc61b18454d8b3b150323fb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "f24a7b28abcb4065aef6dc500514b6e2",
              "pip_warning": {
                "packages": [
                  "whisper"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# First, run this in a cell:\n",
        "!pip uninstall whisper -y\n",
        "!pip uninstall openai-whisper -y\n",
        "!pip install openai-whisper\n",
        "\n",
        "# Then RESTART the runtime (Runtime > Restart runtime in menu)\n",
        "# This is important - the wrong whisper module is cached"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvjFhtq7Eipt",
        "outputId": "7296c34d-6ac2-4243-d22f-6f0c35ebee83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "\n",
            "Checkpoint path: /root/panns_data/Cnn14_mAP=0.431.pth\n",
            "GPU number: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:03<00:00, 41.1MiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔊 Detected sounds:\n",
            "  • Speech: 70.71%\n",
            "  • Vehicle: 61.40%\n",
            "  • Car: 25.56%\n",
            "  • Outside, urban or manmade: 13.38%\n",
            "  • Traffic noise, roadway noise: 13.34%\n",
            "\n",
            "🎤 Transcribing speech...\n",
            "  💬 \"1.5% 1.5% 1.5% 1.5% 1.5%\"\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import whisper\n",
        "from panns_inference import AudioTagging, labels\n",
        "\n",
        "# Initialize models\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using: {device}\\n\")\n",
        "\n",
        "panns_model = AudioTagging(checkpoint_path=None, device=device)\n",
        "whisper_model = whisper.load_model(\"base\", device=device)\n",
        "\n",
        "# Load and prepare audio\n",
        "audio_path = \"/content/city_sounds.mp3\"\n",
        "waveform, sr = torchaudio.load(audio_path)\n",
        "\n",
        "# Resample to 32kHz for PANNs\n",
        "if sr != 32000:\n",
        "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=32000)(waveform)\n",
        "\n",
        "# Convert to mono\n",
        "if waveform.shape[0] > 1:\n",
        "    waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "waveform = waveform.to(device)\n",
        "\n",
        "# Get sound tags\n",
        "clipwise_output, embedding = panns_model.inference(waveform)\n",
        "clipwise_output = clipwise_output.squeeze()\n",
        "top_indices = clipwise_output.argsort()[-5:][::-1]\n",
        "\n",
        "print(\"🔊 Detected sounds:\")\n",
        "for i in top_indices:\n",
        "    print(f\"  • {labels[int(i)]}: {clipwise_output[int(i)]:.2%}\")\n",
        "\n",
        "# Get speech transcript\n",
        "print(\"\\n🎤 Transcribing speech...\")\n",
        "result = whisper_model.transcribe(audio_path)\n",
        "transcript = result[\"text\"].strip()\n",
        "print(f\"  💬 \\\"{transcript}\\\"\" if transcript else \"  (no speech)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDzJZJDDFUJe",
        "outputId": "6c3c0b83-e72a-478d-dd7b-85014d81fa5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/135.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install groq --quiet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCUkL3OYHt6G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gr0Oito5Hpy3",
        "outputId": "7cd55d37-88ba-4c24-8eee-e8b396d9b231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "\n",
            "Checkpoint path: /root/panns_data/Cnn14_mAP=0.431.pth\n",
            "GPU number: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔊 Detected sounds:\n",
            "  • Speech: 70.71%\n",
            "  • Vehicle: 61.40%\n",
            "  • Car: 25.56%\n",
            "  • Outside, urban or manmade: 13.38%\n",
            "  • Traffic noise, roadway noise: 13.34%\n",
            "\n",
            "🎤 Transcribing speech...\n",
            "  💬 \"1.5% 1.5% 1.5% 1.5% 1.5%\"\n",
            "\n",
            "📊 Audio metadata: {'duration': 20.06204081632653, 'sample_rate': 22050, 'channels': 1}\n",
            "\n",
            "✨ Narration: As [car horns blaring] and [chattering voices] fill the air, a passerby yells \"1.5% 1.5% 1.5% 1.5% 1.5%!\" amidst [screeching tires] and [rumbling engines]. [Sirens wailing] in the distance add to the urban chaos. [Revving motorcycles] speed by, weaving through the crowded streets.\n",
            "✅ Audio saved as output.mp3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import whisper\n",
        "from panns_inference import AudioTagging, labels\n",
        "import librosa\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "from groq import Groq\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Initialize models\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using: {device}\\n\")\n",
        "panns_model = AudioTagging(checkpoint_path=None, device=device)\n",
        "whisper_model = whisper.load_model(\"base\", device=device)\n",
        "\n",
        "# Load and prepare audio\n",
        "audio_path = \"/content/city_sounds.mp3\"\n",
        "waveform, sr = torchaudio.load(audio_path)\n",
        "if sr != 32000:\n",
        "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=32000)(waveform)\n",
        "if waveform.shape[0] > 1:\n",
        "    waveform = waveform.mean(dim=0, keepdim=True)\n",
        "waveform = waveform.to(device)\n",
        "\n",
        "# Get sound tags\n",
        "clipwise_output, embedding = panns_model.inference(waveform)\n",
        "clipwise_output = clipwise_output.squeeze()\n",
        "top_indices = clipwise_output.argsort()[-5:][::-1]\n",
        "sound_types = [labels[int(i)] for i in top_indices]\n",
        "print(\"🔊 Detected sounds:\")\n",
        "for i in top_indices:\n",
        "    print(f\"  • {labels[int(i)]}: {clipwise_output[int(i)]:.2%}\")\n",
        "\n",
        "# Get speech transcript\n",
        "print(\"\\n🎤 Transcribing speech...\")\n",
        "result = whisper_model.transcribe(audio_path)\n",
        "transcript = result[\"text\"].strip()\n",
        "print(f\"  💬 \\\"{transcript}\\\"\" if transcript else \"  (no speech)\")\n",
        "\n",
        "# Extract audio metadata\n",
        "audio, sr = librosa.load(audio_path)\n",
        "duration = librosa.get_duration(y=audio, sr=sr)\n",
        "metadata = {\n",
        "    \"duration\": duration,\n",
        "    \"sample_rate\": sr,\n",
        "    \"channels\": 1 if len(audio.shape) == 1 else audio.shape[1]\n",
        "}\n",
        "print(f\"\\n📊 Audio metadata: {metadata}\")\n",
        "\n",
        "# Generate narration with Groq\n",
        "client = Groq(api_key=GROQ_API_KEY)  # Replace with your Groq API key\n",
        "sound_str = \", \".join(sound_types)\n",
        "prompt = (\n",
        "    f\"Write a lively, conversational 2-3 sentence narration of a bustling city scene based on sounds: {sound_str}. \"\n",
        "    f\"Weave in someone casually mentioning '{transcript}' as part of the urban vibe, like a street vendor or passerby. \"\n",
        "    f\"Use vivid sound effect cues in square brackets (e.g., [honking], [chattering voices]) for each sound, avoiding generic terms like 'traffic noise'. \"\n",
        "    f\"Keep it natural, immersive, under 80 words, using metadata {metadata} for context.\"\n",
        ")\n",
        "response = client.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0.4,  # Lower for natural, focused tone\n",
        "    max_tokens=100  # Ensure no cutoff\n",
        ")\n",
        "narration = response.choices[0].message.content\n",
        "with open(\"narration.txt\", \"w\") as f:\n",
        "    f.write(narration)\n",
        "print(f\"\\n✨ Narration: {narration}\")\n",
        "\n",
        "# Convert to speech with ElevenLabs\n",
        "url = \"https://api.elevenlabs.io/v1/text-to-speech/21m00Tcm4TlvDq8ikWAM\"  # Bella voice for warmth\n",
        "headers = {\n",
        "    \"xi-api-key\": ElevenLabs,  # Replace with your ElevenLabs API key\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "payload = {\n",
        "    \"text\": narration,\n",
        "    \"model_id\": \"eleven_multilingual_v2\",\n",
        "    \"voice_settings\": {\"stability\": 0.6, \"similarity_boost\": 0.9, \"style\": 0.2}  # More expressive\n",
        "}\n",
        "response = requests.post(url, headers=headers, json=payload)\n",
        "if response.status_code == 200:\n",
        "    with open(\"temp_output.mp3\", \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    # Amplify audio with pydub\n",
        "    audio = AudioSegment.from_mp3(\"temp_output.mp3\")\n",
        "    audio = audio + 10  # Boost volume by 10dB\n",
        "    audio.export(\"output.mp3\", format=\"mp3\")\n",
        "    print(\"✅ Audio saved as output.mp3\")\n",
        "else:\n",
        "    print(f\"Error: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zPAfOF3FW_m",
        "outputId": "a59a69ce-c0ba-4bac-9e5f-2847d4c2a818"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✨ NARRATION:\n",
            "It sounds like someone's having a conversation in a car, probably stuck in traffic judging by the background noise. One of them just kept repeating \"1.5% 1.5% 1.5%\", I'm not sure what that's about, but it seems like they're trying to make a point.\n"
          ]
        }
      ],
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# Use the sounds we detected\n",
        "sounds = \"Speech, Vehicle, Car, Traffic noise\"\n",
        "transcript = \"1.5% 1.5% 1.5%\"\n",
        "\n",
        "prompt = f\"\"\"Write a casual, natural 2 sentence description of this scene. Be conversational and human, not dramatic.\n",
        "\n",
        "Sounds: {sounds}\n",
        "Someone said: \"{transcript}\"\n",
        "\n",
        "Describe it simply:\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0.7,  # Lower = more natural\n",
        "    max_tokens=80  # Shorter = less flowery\n",
        ")\n",
        "\n",
        "narration = response.choices[0].message.content\n",
        "print(\"\\n✨ NARRATION:\")\n",
        "print(narration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovU1DJFLGV6Q",
        "outputId": "319a0f48-5be3-427b-f963-06364401163d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting edge-tts\n",
            "  Downloading edge_tts-7.2.3-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from edge-tts) (3.13.0)\n",
            "Requirement already satisfied: certifi>=2023.11.17 in /usr/local/lib/python3.12/dist-packages (from edge-tts) (2025.10.5)\n",
            "Requirement already satisfied: tabulate<1.0.0,>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from edge-tts) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from edge-tts) (4.15.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.0->edge-tts) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp<4.0.0,>=3.8.0->edge-tts) (3.11)\n",
            "Downloading edge_tts-7.2.3-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: edge-tts\n",
            "Successfully installed edge-tts-7.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install edge-tts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZgiPJ6QGW4D",
        "outputId": "ee8fce77-7872-4975-eb04-b76ba7409461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Audio saved as output.mp3\n"
          ]
        }
      ],
      "source": [
        "# Save narration to file\n",
        "with open(\"narration.txt\", \"w\") as f:\n",
        "    f.write(narration)\n",
        "\n",
        "# Convert to speech with Edge TTS\n",
        "!edge-tts --voice en-US-GuyNeural --text \"$(cat narration.txt)\" --write-media output.mp3\n",
        "\n",
        "print(\"✅ Audio saved as output.mp3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydH20PGYHTI4"
      },
      "outputs": [],
      "source": [
        "client = Groq(api_key=GROQ_API_KEY)  # Replace with your Groq API key\n",
        "sound_str = \", \".join(sound_types)\n",
        "transcript = \"1.5% 1.5% 1.5%\"\n",
        "prompt = (\n",
        "    f\"Write a natural, conversational 2-3 sentence narration of a vivid urban scene based on these sounds: {sound_str}. \"\n",
        "    f\"Include metadata: {metadata}. Mention someone saying '{transcript}' naturally in the scene. \"\n",
        "    f\"Add dramatic sound effect cues in square brackets (e.g., [traffic noise]) for each sound. \"\n",
        "    f\"Keep it immersive but human-like, under 80 words, for text-to-speech.\"\n",
        ")\n",
        "response = client.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0.6,\n",
        "    max_tokens=80\n",
        ")\n",
        "narration = response.choices[0].message.content\n",
        "with open(\"narration.txt\", \"w\") as f:\n",
        "    f.write(narration)\n",
        "print(f\"\\n✨ Narration: {narration}\")\n",
        "\n",
        "# Convert to speech with ElevenLabs\n",
        "url = \"https://api.elevenlabs.io/v1/text-to-speech/pNInz6obpgDQGcFmaJgB\"  # Adam voice\n",
        "headers = {\n",
        "    \"xi-api-key\": \"your_elevenlabs_api_key\",  # Replace with your ElevenLabs API key\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "payload = {\n",
        "    \"text\": narration,\n",
        "    \"model_id\": \"eleven_monolingual_v1\",\n",
        "    \"voice_settings\": {\"stability\": 0.5, \"similarity_boost\": 0.5}\n",
        "}\n",
        "response = requests.post(url, headers=headers, json=payload)\n",
        "if response.status_code == 200:\n",
        "    with open(\"output.mp3\", \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(\"✅ Audio saved as output.mp3\")\n",
        "else:\n",
        "    print(f\"Error: {response.status_code} - {response.text}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
