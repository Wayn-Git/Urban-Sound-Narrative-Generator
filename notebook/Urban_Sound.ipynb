{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "audio_path = \"/content/city_sounds.mp3\""
      ],
      "metadata": {
        "id": "-SKbMNS_8RiI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the Libraries\n",
        "\n",
        "!pip uninstall --quiet whisper -y\n",
        "!pip uninstall --quiet openai-whisper -y\n",
        "!pip install --quiet panns-inference openai-whisper torch torchaudio groq librosa numpy requests pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvXuymoj8Y2g",
        "outputId": "6526a17c-76ed-41ef-8135-d15ef09e8c8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping whisper as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping openai-whisper as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# API Keys\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "ELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\")\n",
        "RUNWAYML_API_SECRET = os.getenv(\"RUNWAYML_API_SECRET\")"
      ],
      "metadata": {
        "id": "trlZiJvkLW53"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import whisper\n",
        "from panns_inference import AudioTagging, labels\n",
        "import librosa\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "from groq import Groq\n",
        "from pydub import AudioSegment"
      ],
      "metadata": {
        "id": "DCUkL3OYHt6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b9bd033-efd0-4c1e-961d-9317d7923c91"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using: {device}\\n\")\n",
        "panns_model = AudioTagging(checkpoint_path=None, device=device)\n",
        "whisper_model = whisper.load_model(\"base\", device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGH2hyBXKb29",
        "outputId": "264b7a66-a28c-4aac-af88-e8d15ebbd1c6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cuda\n",
            "\n",
            "Checkpoint path: /root/panns_data/Cnn14_mAP=0.431.pth\n",
            "GPU number: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139M/139M [00:00<00:00, 183MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare audio\n",
        "audio_path = \"/content/city_sounds.mp3\"\n",
        "waveform, sr = torchaudio.load(audio_path)\n",
        "if sr != 32000:\n",
        "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=32000)(waveform)\n",
        "if waveform.shape[0] > 1:\n",
        "    waveform = waveform.mean(dim=0, keepdim=True)\n",
        "waveform = waveform.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdJ5BovKKcN-",
        "outputId": "433bbc9f-50f4-4450-9d95-25f8f10a6747"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get sound tags\n",
        "clipwise_output, embedding = panns_model.inference(waveform)\n",
        "clipwise_output = clipwise_output.squeeze()\n",
        "top_indices = clipwise_output.argsort()[-5:][::-1]\n",
        "sound_types = [labels[int(i)] for i in top_indices]\n",
        "print(\"üîä Detected sounds:\")\n",
        "for i in top_indices:\n",
        "    print(f\"  ‚Ä¢ {labels[int(i)]}: {clipwise_output[int(i)]:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrgLBUn3KftJ",
        "outputId": "45c4a681-31ab-4dd6-cabf-5828bd56fef1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîä Detected sounds:\n",
            "  ‚Ä¢ Speech: 70.71%\n",
            "  ‚Ä¢ Vehicle: 61.40%\n",
            "  ‚Ä¢ Car: 25.56%\n",
            "  ‚Ä¢ Outside, urban or manmade: 13.38%\n",
            "  ‚Ä¢ Traffic noise, roadway noise: 13.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get speech transcript via whisper open ai\n",
        "print(\"\\nüé§ Transcribing speech...\")\n",
        "result = whisper_model.transcribe(audio_path)\n",
        "transcript = result[\"text\"].strip()\n",
        "print(f\"  üí¨ \\\"{transcript}\\\"\" if transcript else \"  (no speech)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP-ourjnKkYW",
        "outputId": "4d2d1b1a-a883-4143-afa4-de2ec124905c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üé§ Transcribing speech...\n",
            "  üí¨ \"1.5% 1.5% 1.5% 1.5% 1.5%\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate narration with Groq\n",
        "client = Groq(api_key=GROQ_API_KEY)  # Replace with your Groq API key\n",
        "\n",
        "sound_str = \", \".join(sound_types)\n",
        "\n",
        "prompt = (\n",
        "f\"Write a lively, conversational 2-3 sentence narration of a bustling city scene, capturing the urban vibe based on sounds: {sound_str}. Seamlessly integrate a passerby or vendor casually saying '{transcript}' as part of the scene, using square brackets (e.g., [thoughtfully], [sarcastically], [enthusiastically]) to denote emotional voice tones for expressive dialogue delivery. Ensure the narration is natural, immersive, and reflects the full essence of the audio through vivid emotional expression, without including sound effect descriptions in the text.\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0.4,  # Lower for natural, focused tone\n",
        "    max_tokens=100  # Ensure no cutoff\n",
        ")\n",
        "narration = response.choices[0].message.content\n",
        "with open(\"narration.txt\", \"w\") as f:\n",
        "    f.write(narration)\n",
        "print(f\"\\n‚ú® Narration: {narration}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRL0JVshOQfb",
        "outputId": "7df51d46-9a3c-44d6-bbbe-7aeccea8ce8b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ú® Narration: As people rushed to and fro, a vendor [excitedly] called out to passersby, \"1.5% 1.5% 1.5% 1.5% 1.5%,\" trying to entice them with a great deal, while a pedestrian [thoughtfully] muttered to herself about the latest interest rates. The city was alive with the hum of activity, as strangers became temporary companions in the shared experience of navigating the crowded streets. Amidst\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet runwayml"
      ],
      "metadata": {
        "id": "Bupvg7CLOgsW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from typing import Optional\n",
        "\n",
        "# Configure logging for debugging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def convert_text_to_speech(\n",
        "    text: str,\n",
        "    voice_id: str = \"21m00Tcm4TlvDq8ikWAM\",  # Default: Bella for warmth\n",
        "    model_id: str = \"eleven_multilingual_v3\",  # Latest expressive model\n",
        "    api_key: str = os.getenv(\"ELEVENLABS_API_KEY\"),  # Secure API key\n",
        "    output_file: str = \"output.mp3\",\n",
        "    volume_boost_db: float = 8.0,  # Balanced volume boost\n",
        "    stability: float = 0.4,  # Lower for emotional expressiveness\n",
        "    similarity_boost: float = 0.8  # Optimize voice clarity\n",
        ") -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Convert emotionally tagged text to speech using ElevenLabs v3 API.\n",
        "\n",
        "    Args:\n",
        "        text: Narration with emotional tags (e.g., [sarcastically], [enthusiastically]).\n",
        "        voice_id: ElevenLabs voice ID.\n",
        "        model_id: ElevenLabs model ID (e.g., eleven_multilingual_v3).\n",
        "        api_key: API key from environment variable.\n",
        "        output_file: Path for output audio file.\n",
        "        volume_boost_db: Volume increase in decibels.\n",
        "        stability: Voice stability (0.0-1.0, lower for more emotion).\n",
        "        similarity_boost: Voice clarity (0.0-1.0).\n",
        "\n",
        "    Returns:\n",
        "        Path to saved audio file or None if failed.\n",
        "    \"\"\"\n",
        "    # Validate inputs\n",
        "    if not api_key:\n",
        "        logger.error(\"Missing ElevenLabs API key. Set ELEVENLABS_API_KEY environment variable.\")\n",
        "        return None\n",
        "    if not text.strip():\n",
        "        logger.error(\"Input text is empty.\")\n",
        "        return None\n",
        "\n",
        "    # API request setup\n",
        "    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}\"\n",
        "    headers = {\n",
        "        \"xi-api-key\": ,\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Accept\": \"audio/mpeg\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"text\": text,\n",
        "        \"model_id\": model_id,\n",
        "        \"voice_settings\": {\n",
        "            \"stability\": stability,\n",
        "            \"similarity_boost\": similarity_boost,\n",
        "            \"style\": 0.5,  # Balanced expressiveness for tagged emotions\n",
        "            \"use_speaker_boost\": True  # Enhance clarity\n",
        "        }\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Send request to ElevenLabs API\n",
        "        response = requests.post(url, headers=headers, json=payload, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Save temporary audio\n",
        "        temp_file = \"temp_output.mp3\"\n",
        "        with open(temp_file, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        logger.info(f\"Temporary audio saved as {temp_file}\")\n",
        "\n",
        "        # Process audio with pydub\n",
        "        try:\n",
        "            audio = AudioSegment.from_mp3(temp_file)\n",
        "            audio = audio + volume_boost_db\n",
        "            audio = audio.normalize()  # Prevent clipping\n",
        "            audio.export(output_file, format=\"mp3\", bitrate=\"192k\")  # High-quality output\n",
        "            logger.info(f\"‚úÖ Audio saved as {output_file}\")\n",
        "\n",
        "            # Clean up\n",
        "            Path(temp_file).unlink(missing_ok=True)\n",
        "            return output_file\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Audio processing failed: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"API request failed: {response.status_code if 'response' in locals() else 'No response'} - {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    # Placeholder for your model's narration output\n",
        "    narration = narration  # Replace with your model's output\n",
        "    output = convert_text_to_speech(\n",
        "        text=narration,\n",
        "        output_file=\"city_narration.mp3\"\n",
        "    )\n",
        "    if output:\n",
        "        logger.info(f\"Audio generated: {output}\")\n",
        "    else:\n",
        "        logger.error(\"Audio generation failed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2UAIJMBHVjIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "headers = {\"xi-api-key\": ELEVENLABS_API_KEY}\n",
        "response = requests.get(\"https://api.elevenlabs.io/v1/voices\", headers=headers)\n",
        "voices = response.json()[\"voices\"]  # List of dicts with 'voice_id' and 'name'\n",
        "voice_ids = [v[\"voice_id\"] for v in voices]\n",
        "print(voice_ids)  # Outputs all IDs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qLaVEA-WifW",
        "outputId": "65bf596b-be09-43a3-9f17-23f07c86e17f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2EiwWnXFnvU5JabPnv8n', 'CwhRBWXzGAHq8TQ4Fs17', 'EXAVITQu4vr4xnSDxMaL', 'FGY2WhTYpPnrIDTdsKH5', 'IKne3meq5aSn9XLyUdCD', 'JBFqnCBsd6RMkjVDRZzb', 'N2lVS1w4EtoT3dr4eOWO', 'SAz9YHcvj6GT2YYXdXww', 'SOYHLrjzK2X1ezoPC6cr', 'TX3LPaxmHKxFdv7VOQHJ', 'Xb7hH8MSUJpSbSDYk0k2', 'XrExE9yKIg1WjnnlVkGX', 'bIHbv24MWmeRgasZH58o', 'cgSgspJ2msm6clMCkdW9', 'cjVigY5qzO86Huf0OWal', 'iP95p4xoKVk53GoZ742B', 'nPczCjzI2devNBz1zQrb', 'onwK4e9ZLuTAKqWW03F9', 'pFZP5JQG7iQjIQuC4Bku', 'pqHfZKP75CvOlQylNhV4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "url = \"https://api.elevenlabs.io/v1/text-to-speech/cgSgspJ2msm6clMCkdW9\"  # Jessica voice\n",
        "headers = {\n",
        "    \"xi-api-key\": ELEVENLABS_API_KEY,\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Accept\": \"audio/mpeg\"\n",
        "}\n",
        "payload = {\n",
        "    \"text\": narration,  # model's narration with emotional tags\n",
        "    \"model_id\": \"eleven_v3\",  # v3 for emotional tags\n",
        "    \"voice_settings\": {\n",
        "        \"stability\": 0.0,  # Creative mode for emotional expressiveness\n",
        "        \"similarity_boost\": 0.85,  # Enhance voice clarity\n",
        "        \"style\": 0.6,  # Balanced for emotional nuance\n",
        "        \"use_speaker_boost\": True  # Boost clarity for Jessica‚Äôs voice\n",
        "    }\n",
        "}\n",
        "\n",
        "# Send request to ElevenLabs API\n",
        "try:\n",
        "    response = requests.post(url, headers=headers, json=payload, timeout=30)\n",
        "    if response.status_code == 200:\n",
        "        # Save temporary audio\n",
        "        with open(\"temp_output.mp3\", \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        # Amplify and normalize audio with pydub\n",
        "        audio = AudioSegment.from_mp3(\"temp_output.mp3\")\n",
        "        audio = audio + 8  # 8dB boost for balance\n",
        "        audio = audio.normalize()  # Prevent clipping\n",
        "        audio.export(\"output.mp3\", format=\"mp3\", bitrate=\"192k\")  # High-quality output\n",
        "        os.remove(\"temp_output.mp3\")  # Clean up\n",
        "        print(\"‚úÖ Audio saved as output.mp3\")\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code} - {response.text}\")\n",
        "except Exception as e:\n",
        "    print(f\"Request failed: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gr0Oito5Hpy3",
        "outputId": "ca46f5ba-c680-4b63-ed1f-03984f147a4d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Audio saved as output.mp3\n"
          ]
        }
      ]
    }
  ]
}