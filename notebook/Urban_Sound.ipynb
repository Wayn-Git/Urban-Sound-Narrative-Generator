{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-SKbMNS_8RiI"
      },
      "outputs": [],
      "source": [
        "audio_path = \"/content/city_sounds.mp3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvXuymoj8Y2g",
        "outputId": "6526a17c-76ed-41ef-8135-d15ef09e8c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping whisper as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping openai-whisper as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Installing the Libraries\n",
        "\n",
        "!pip uninstall --quiet whisper -y\n",
        "!pip uninstall --quiet openai-whisper -y\n",
        "!pip install --quiet panns-inference openai-whisper torch torchaudio groq librosa numpy requests pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "trlZiJvkLW53"
      },
      "outputs": [],
      "source": [
        "# API Keys\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "ELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\")\n",
        "RUNWAYML_API_SECRET = os.getenv(\"RUNWAYML_API_SECRET\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCUkL3OYHt6G",
        "outputId": "2b9bd033-efd0-4c1e-961d-9317d7923c91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import whisper\n",
        "from panns_inference import AudioTagging, labels\n",
        "import librosa\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "from groq import Groq\n",
        "from pydub import AudioSegment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGH2hyBXKb29",
        "outputId": "264b7a66-a28c-4aac-af88-e8d15ebbd1c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "\n",
            "Checkpoint path: /root/panns_data/Cnn14_mAP=0.431.pth\n",
            "GPU number: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139M/139M [00:00<00:00, 183MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# Initialize models\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using: {device}\\n\")\n",
        "panns_model = AudioTagging(checkpoint_path=None, device=device)\n",
        "whisper_model = whisper.load_model(\"base\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdJ5BovKKcN-",
        "outputId": "433bbc9f-50f4-4450-9d95-25f8f10a6747"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        }
      ],
      "source": [
        "# Load and prepare audio\n",
        "audio_path = \"/content/city_sounds.mp3\"\n",
        "waveform, sr = torchaudio.load(audio_path)\n",
        "if sr != 32000:\n",
        "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=32000)(waveform)\n",
        "if waveform.shape[0] > 1:\n",
        "    waveform = waveform.mean(dim=0, keepdim=True)\n",
        "waveform = waveform.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrgLBUn3KftJ",
        "outputId": "45c4a681-31ab-4dd6-cabf-5828bd56fef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîä Detected sounds:\n",
            "  ‚Ä¢ Speech: 70.71%\n",
            "  ‚Ä¢ Vehicle: 61.40%\n",
            "  ‚Ä¢ Car: 25.56%\n",
            "  ‚Ä¢ Outside, urban or manmade: 13.38%\n",
            "  ‚Ä¢ Traffic noise, roadway noise: 13.34%\n"
          ]
        }
      ],
      "source": [
        "# Get sound tags\n",
        "clipwise_output, embedding = panns_model.inference(waveform)\n",
        "clipwise_output = clipwise_output.squeeze()\n",
        "top_indices = clipwise_output.argsort()[-5:][::-1]\n",
        "sound_types = [labels[int(i)] for i in top_indices]\n",
        "print(\"üîä Detected sounds:\")\n",
        "for i in top_indices:\n",
        "    print(f\"  ‚Ä¢ {labels[int(i)]}: {clipwise_output[int(i)]:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP-ourjnKkYW",
        "outputId": "4d2d1b1a-a883-4143-afa4-de2ec124905c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üé§ Transcribing speech...\n",
            "  üí¨ \"1.5% 1.5% 1.5% 1.5% 1.5%\"\n"
          ]
        }
      ],
      "source": [
        "# Get speech transcript via whisper open ai\n",
        "print(\"\\nüé§ Transcribing speech...\")\n",
        "result = whisper_model.transcribe(audio_path)\n",
        "transcript = result[\"text\"].strip()\n",
        "print(f\"  üí¨ \\\"{transcript}\\\"\" if transcript else \"  (no speech)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRL0JVshOQfb",
        "outputId": "7df51d46-9a3c-44d6-bbbe-7aeccea8ce8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® Narration: As people rushed to and fro, a vendor [excitedly] called out to passersby, \"1.5% 1.5% 1.5% 1.5% 1.5%,\" trying to entice them with a great deal, while a pedestrian [thoughtfully] muttered to herself about the latest interest rates. The city was alive with the hum of activity, as strangers became temporary companions in the shared experience of navigating the crowded streets. Amidst\n"
          ]
        }
      ],
      "source": [
        "# Generate narration with Groq\n",
        "client = Groq(api_key=GROQ_API_KEY)  # Replace with your Groq API key\n",
        "\n",
        "sound_str = \", \".join(sound_types)\n",
        "\n",
        "prompt = (\n",
        "f\"Write a lively, conversational 2-3 sentence narration of a bustling city scene, capturing the urban vibe based on sounds: {sound_str}. Seamlessly integrate a passerby or vendor casually saying '{transcript}' as part of the scene, using square brackets (e.g., [thoughtfully], [sarcastically], [enthusiastically]) to denote emotional voice tones for expressive dialogue delivery. Ensure the narration is natural, immersive, and reflects the full essence of the audio through vivid emotional expression, without including sound effect descriptions in the text.\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0.4,  # Lower for natural, focused tone\n",
        "    max_tokens=100  # Ensure no cutoff\n",
        ")\n",
        "narration = response.choices[0].message.content\n",
        "with open(\"narration.txt\", \"w\") as f:\n",
        "    f.write(narration)\n",
        "print(f\"\\n‚ú® Narration: {narration}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gr0Oito5Hpy3",
        "outputId": "ca46f5ba-c680-4b63-ed1f-03984f147a4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Audio saved as output.mp3\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "url = \"https://api.elevenlabs.io/v1/text-to-speech/cgSgspJ2msm6clMCkdW9\"  # Jessica voice\n",
        "headers = {\n",
        "    \"xi-api-key\": ELEVENLABS_API_KEY,\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Accept\": \"audio/mpeg\"\n",
        "}\n",
        "payload = {\n",
        "    \"text\": narration,  # model's narration with emotional tags\n",
        "    \"model_id\": \"eleven_v3\",  # v3 for emotional tags\n",
        "    \"voice_settings\": {\n",
        "        \"stability\": 0.0,  # Creative mode for emotional expressiveness\n",
        "        \"similarity_boost\": 0.85,  # Enhance voice clarity\n",
        "        \"style\": 0.6,  # Balanced for emotional nuance\n",
        "        \"use_speaker_boost\": True  # Boost clarity for Jessica‚Äôs voice\n",
        "    }\n",
        "}\n",
        "\n",
        "# Send request to ElevenLabs API\n",
        "try:\n",
        "    response = requests.post(url, headers=headers, json=payload, timeout=30)\n",
        "    if response.status_code == 200:\n",
        "        # Save temporary audio\n",
        "        with open(\"temp_output.mp3\", \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        # Amplify and normalize audio with pydub\n",
        "        audio = AudioSegment.from_mp3(\"temp_output.mp3\")\n",
        "        audio = audio + 8  # 8dB boost for balance\n",
        "        audio = audio.normalize()  # Prevent clipping\n",
        "        audio.export(\"output.mp3\", format=\"mp3\", bitrate=\"192k\")  # High-quality output\n",
        "        os.remove(\"temp_output.mp3\")  # Clean up\n",
        "        print(\"‚úÖ Audio saved as output.mp3\")\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code} - {response.text}\")\n",
        "except Exception as e:\n",
        "    print(f\"Request failed: {str(e)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
